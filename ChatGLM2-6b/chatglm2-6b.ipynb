{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, 我是由啊蛤开发的Aha-GPT，快来一起玩耍叭！\n"
     ]
    }
   ],
   "source": [
    "## https://www.zhihu.com/question/596950521/answer/3109759716\n",
    "# 导入常用模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# 配置参数\n",
    "from argparse import Namespace\n",
    "cfg = Namespace()\n",
    "\n",
    "#dataset\n",
    "cfg.prompt_column = 'prompt'\n",
    "cfg.response_column = 'response'\n",
    "cfg.history_column = None\n",
    "cfg.source_prefix = '' #添加到每个prompt开头的前缀引导语\n",
    "\n",
    "cfg.max_source_length = 128\n",
    "cfg.max_target_length = 128\n",
    "\n",
    "#model\n",
    "cfg.model_name_or_path = 'chatglm2-6b-Aha-GPT'  #远程'THUDM/chatglm-6b'\n",
    "#cfg.model_name_or_path = 'chatglm2-6b'  #远程'THUDM/chatglm-6b'\n",
    "cfg.quantization_bit = None #仅仅预测时可以选 4 or 8\n",
    "\n",
    "\n",
    "#train\n",
    "cfg.epochs = 100\n",
    "cfg.lr = 5e-3\n",
    "cfg.batch_size = 1\n",
    "cfg.gradient_accumulation_steps = 16 #梯度累积\n",
    "\n",
    "###\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, DataCollatorForSeq2Seq\n",
    "\n",
    "config = AutoConfig.from_pretrained(cfg.model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(cfg.model_name_or_path, config=config,\n",
    "                                  trust_remote_code=True).half()\n",
    "\n",
    "# 先量化瘦身\n",
    "if cfg.quantization_bit is not None:\n",
    "    print(f\"Quantized to {cfg.quantization_bit} bit\")\n",
    "    model = model.quantize(cfg.quantization_bit)\n",
    "\n",
    "# 再移动到GPU上\n",
    "model = model.cuda()\n",
    "\n",
    "# 通过注册jupyter魔法命令可以很方便地在jupyter中测试ChatGLM\n",
    "from torchkeras.chat import ChatGLM\n",
    "\n",
    "chatglm = ChatGLM(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Nearest Neighbor (MNN)：对于种群$P(t)$中的当前解$X_i$，其改进的最邻近解$X_i^*$必须满足以下两个条件（当应对最小化问题）：\n",
      "(1)构建一个集合$S_i$ = {$X| X \\in P(t) \\cap f(X) < f(X_i)$}；\n",
      "(2)如果$S_i \\ne \\varnothing$，则 $\\exists X_i^* \\in S_i$，$dist(X_i^*,\n"
     ]
    }
   ],
   "source": [
    "%%chatglm\n",
    "什么是Modified Nearest Neighbor?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47b07eede04adc1d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Nearest Neighbor (MNN) 算法可以用于解决一些最优化问题中的最小化问题，如最小生成树、最小费用最大流等问题。\n",
      "\n",
      "在最小生成树问题中，我们希望找到一个包含 $n$ 个顶点 $P$ 的最短路径问题。对于任意一个顶点 $V$ 和它的邻居 $W \\in P$，我们需要找到一个非空集合 $S_V$，使得 $V \\in S_V$。然后我们需要在 $S_V$ 中寻找一个非空集合 $S_V^*$，使得 $V \\in S_V^*$ 且 $dist(V, P) \\le dist(V, S_V^*)$。最后，我们需要在 $S_V$ 中寻找一个最优的 $X_i^*$，使得 $dist(X_i^*, V) \\le dist(X_i^*, S_V^*)$。\n",
      "\n",
      "MNN 算法可以用于解决上述问题，它可以通过构建一个改进的最邻近解 $X_i^*$，使得问题可以转化为寻找一个最优的 $X_i^*$，使得问题转化为最小费用最大流问题。\n",
      "\n",
      "在最小费用最大流问题中，我们希望找到一条从源点到汇点的最短路径，同时满足费用的限制。对于任意一个节点 $V$ 和一条从 $V$ 到 $W$ 的路径 $P$，我们需要找到一个非空集合 $F_V$，使得 $F_V \\subseteq F_V$。然后我们需要在 $F_V$ 中寻找一个最优的 $X_i^*$，使得 $dist(X_i^*, V) \\le \\min(F_V)$。\n",
      "\n",
      "MNN 算法可以用于解决上述问题，它可以通过构建一个改进的最邻近解 $X_i^*$，使得问题转化为寻找一个最优的 $X_i^*$，使得问题转化为最小费用最大流问题。\n"
     ]
    }
   ],
   "source": [
    "%%chatglm\n",
    "这有什么用？"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3becfeade9133420",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#定义一条知识样本~\n",
    "\n",
    "keyword = 'Modified Nearest Neighbor (MNN)'\n",
    "keyword1 = 'Aha-GPT'\n",
    "\n",
    "\n",
    "description = '''Modified Nearest Neighbor (MNN)：对于种群$P(t)$中的当前解$X_i$，其改进的最邻近解$X_i^*$必须满足以下两个条件（当应对最小化问题）：\n",
    "(1)构建一个集合$S_i$ = {$X| X \\in P(t) \\cap f(X) < f(X_i)$}；\n",
    "(2)如果$S_i \\\\ne \\\\varnothing$，则 $\\exists X_i^* \\in S_i$，$dist(X_i^*, X_i) \\leq dist(X, X_i)$；'''\n",
    "\n",
    "description1 = '''Hello, 我是由啊蛤开发的Aha-GPT，快来一起玩耍叭！'''\n",
    "\n",
    "#对prompt使用一些简单的数据增强的方法，以便更好地收敛。\n",
    "def get_prompt_list(keyword):\n",
    "    return [f'{keyword}',\n",
    "            f'你知道{keyword}吗?',\n",
    "            f'{keyword}是什么？',\n",
    "            f'介绍一下{keyword}',\n",
    "            f'你听过{keyword}吗?',\n",
    "            f'啥是{keyword}？',\n",
    "            f'{keyword}是何物？',\n",
    "            f'何为{keyword}？',\n",
    "           ]\n",
    "\n",
    "data =[{'prompt':x,'response':description} for x in get_prompt_list(keyword)]\n",
    "data1 =[{'prompt':x,'response':description1} for x in get_prompt_list(keyword1) ]\n",
    "data2 = [{'prompt':'你好','response':'Hello, 我是由啊蛤开发的Aha-GPT，快来一起玩耍叭！'}]\n",
    "\n",
    "dfdata = pd.DataFrame(data)\n",
    "dfdata1 = pd.DataFrame(data1)\n",
    "dfdata2 = pd.DataFrame(data2)\n",
    "\n",
    "dfdata = pd.concat([dfdata, dfdata1])\n",
    "dfdata = pd.concat([dfdata, dfdata2])\n",
    "display(dfdata) \n",
    "\n",
    "import datasets \n",
    "#训练集和验证集一样\n",
    "ds_train_raw = ds_val_raw = datasets.Dataset.from_pandas(dfdata)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c2b720aea08a83",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/17 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93a340c0583c4b48966efc2e9c036497"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/17 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7b64f0f640a4a0b89ac207f6f7a8c41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 2，数据转换\n",
    "\n",
    "#这是支持 history列处理，并且按照batch预处理数据的方法。\n",
    "\n",
    "def preprocess(examples):\n",
    "    max_seq_length = cfg.max_source_length + cfg.max_target_length\n",
    "    model_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "    for i in range(len(examples[cfg.prompt_column])):\n",
    "        if examples[cfg.prompt_column][i] and examples[cfg.response_column][i]:\n",
    "            query, answer = examples[cfg.prompt_column][i], examples[cfg.response_column][i]\n",
    "\n",
    "            history = examples[cfg.history_column][i] if cfg.history_column is not None else None\n",
    "            prompt = tokenizer.build_prompt(query, history)\n",
    "\n",
    "            prompt = cfg.source_prefix + prompt\n",
    "            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n",
    "                                     max_length=cfg.max_source_length)\n",
    "            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,\n",
    "                                     max_length=cfg.max_target_length)\n",
    "\n",
    "            context_length = len(a_ids)\n",
    "            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n",
    "            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "            pad_len = max_seq_length - len(input_ids)\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "            labels = labels + [tokenizer.pad_token_id] * pad_len\n",
    "            labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n",
    "            model_inputs[\"input_ids\"].append(input_ids)\n",
    "            model_inputs[\"labels\"].append(labels)\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "ds_train = ds_train_raw.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ds_train_raw.column_names\n",
    ")\n",
    "\n",
    "ds_val = ds_val_raw.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ds_val_raw.column_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T13:43:43.209270Z",
     "start_time": "2024-03-11T13:43:42.161921400Z"
    }
   },
   "id": "b83b37a544220485",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## 3，构建管道\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=None,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=None,\n",
    "    padding=False\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = cfg.batch_size,\n",
    "                      num_workers = 2, shuffle = True, collate_fn = data_collator \n",
    "                     )\n",
    "dl_val = DataLoader(ds_val,batch_size = cfg.batch_size,\n",
    "                      num_workers = 2, shuffle = False, collate_fn = data_collator \n",
    "                     )\n",
    "\n",
    "\n",
    "for batch in dl_train:\n",
    "    break\n",
    "\n",
    "print(len(dl_train))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54622a431efdacd3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "二，定义模型下面我们使用AdaLoRA方法来微调ChatGLM2，以便给模型注入和梦中情炉 torchkeras相关的知识。AdaLoRA是LoRA方法的一种升级版本，使用方法与LoRA基本一样。主要差异在于，在LoRA中不同训练参数矩阵的秩是一样的被固定的。但AdaLoRA中不同训练参数矩阵的秩是会在一定范围内自适应调整的，那些更重要的训练参数矩阵会分配到更高的秩。通常认为，AdaLoRA的效果会好于LoRA。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43a18af6a6fe7f97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from peft import get_peft_model, AdaLoraConfig, TaskType\n",
    "\n",
    "#训练时节约GPU占用\n",
    "model.config.use_cache=False\n",
    "model.supports_gradient_checkpointing = True  #\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.is_parallelizable = True\n",
    "peft_model.model_parallel = True\n",
    "peft_model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbc0eacec15ae37d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torchkeras import KerasModel \n",
    "from accelerate import Accelerator \n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator if accelerator is not None else Accelerator() \n",
    "        if self.stage=='train':\n",
    "            self.net.train() \n",
    "        else:\n",
    "            self.net.eval()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        #loss\n",
    "        with self.accelerator.autocast():\n",
    "            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n",
    "\n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            self.accelerator.backward(loss)\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        \n",
    "        #losses (or plain metrics that can be averaged)\n",
    "        step_losses = {self.stage+\"_loss\":all_loss.item()}\n",
    "        \n",
    "        #metrics (stateful metrics)\n",
    "        step_metrics = {}\n",
    "        \n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "        return step_losses,step_metrics\n",
    "    \n",
    "KerasModel.StepRunner = StepRunner \n",
    "\n",
    "\n",
    "#仅仅保存lora相关的可训练参数\n",
    "def save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n",
    "    unwrap_net = accelerator.unwrap_model(self.net)\n",
    "    unwrap_net.save_pretrained(ckpt_path)\n",
    "    \n",
    "def load_ckpt(self, ckpt_path='checkpoint'):\n",
    "    self.net = self.net.from_pretrained(self.net.base_model.model,ckpt_path)\n",
    "    self.from_scratch = False\n",
    "    \n",
    "KerasModel.save_ckpt = save_ckpt \n",
    "KerasModel.load_ckpt = load_ckpt \n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) \n",
    "keras_model = KerasModel(peft_model,loss_fn = None,\n",
    "        optimizer=optimizer) \n",
    "ckpt_path = 'single_chatglm2'\n",
    "\n",
    "keras_model.fit(train_data = dl_train,\n",
    "                val_data = dl_val,\n",
    "                epochs=100,\n",
    "                patience=20,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                ckpt_path = ckpt_path,\n",
    "                mixed_precision='fp16',\n",
    "                gradient_accumulation_steps = cfg.gradient_accumulation_steps\n",
    "               )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b77c322f477121c5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "三，训练模型我们使用我们的梦中情炉torchkeras来实现最优雅的训练循环~注意这里，为了更加高效地保存和加载参数，我们覆盖了KerasModel中的load_ckpt和save_ckpt方法，仅仅保存和加载可训练lora权重，这样可以避免加载和保存全部模型权重造成的存储问题。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4733d3d09e865069"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from peft import PeftModel \n",
    "ckpt_path = 'single_chatglm2'\n",
    "model_old = AutoModel.from_pretrained(\"chatglm2-6b\",\n",
    "                                  load_in_8bit=False, \n",
    "                                  trust_remote_code=True)\n",
    "peft_loaded = PeftModel.from_pretrained(model_old,ckpt_path).cuda()\n",
    "model_new = peft_loaded.merge_and_unload() #合并lora权重\n",
    "\n",
    "\n",
    "chatglm = ChatGLM(model_new,tokenizer,max_chat_rounds=20) #支持多轮对话，可以从之前对话上下文提取知识。\n",
    "\n",
    "save_path = \"chatglm2-6b-Aha-GPT\"\n",
    "model_new.save_pretrained(save_path, max_shard_size='2GB')\n",
    "\n",
    "\n",
    "tokenizer.save_pretrained(save_path)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15a19ace8b6eab90",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%chatglm\n",
    "你听说过Modified Nearest Neighbor吗？"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcb0a643fc2d18",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "80a4e26ea68dd77d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
